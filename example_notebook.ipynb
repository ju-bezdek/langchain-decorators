{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Decorators âœ¨\n",
    "\n",
    "lanchchain decorators is a layer on the top op LangChain that provides syntactic sugar for writing custom langchain prompts and chains\n",
    "\n",
    "This notebook shows you what and how\n",
    " \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "> All the cells are designed to run on its own, except this initialization cell that is required to install the packages and setup override some default settings for more verbose logging for demonstration purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install langchain_decorators (will install also langchain and promptwatch)\n",
    "!pip install langchain_decorators\n",
    "\n",
    "#########################################\n",
    "# you need to setup your openai api key\n",
    "#########################################\n",
    "%env OPENAI_API_KEY=#sk-********************************  \n",
    "\n",
    "\n",
    "import os\n",
    "if not os.environ[\"OPENAI_API_KEY\"]:\n",
    "    raise Exception(\"You need to setup your openai api key\")\n",
    "\n",
    "from langchain_decorators import GlobalSettings\n",
    "\n",
    "import logging\n",
    "\n",
    "# let's define our settings, just to make it more verbose for demonstration\n",
    "GlobalSettings.define_settings(\n",
    "    #default_llm=ChatOpenAI(temperature=0.0), this is default... can change it here globally\n",
    "    #default_streaming_llm=ChatOpenAI(temperature=0.0,streaming=True), this is default... can change it here for all ... will be used for streaming\n",
    "    logging_level=logging.INFO, \n",
    "    print_prompt=True, \n",
    "    print_prompt_name=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running simple prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[40m\n",
      "Result:\n",
      "\"Introducing the Revolutionary Twitter App: Unleash Real Magic with Our Cutting-Edge Technology!\"\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"Introducing the Revolutionary Twitter App: Unleash Real Magic with Our Cutting-Edge Technology!\"'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this code example is complete and should run as it is\n",
    "\n",
    "from langchain_decorators import llm_prompt\n",
    "\n",
    "@llm_prompt\n",
    "def write_me_short_post(topic:str, platform:str=\"twitter\", audience:str = \"developers\"):\n",
    "    \"\"\"\n",
    "    Write me a short header for my post about {topic} for {platform} platform. \n",
    "    It should be for {audience} audience.\n",
    "    (Max 15 words)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "write_me_short_post(topic=\"Releasing a new App that can do real magic!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple script with async and streaming\n",
    "\n",
    "If we wan't to leverage streaming:\n",
    " - we need to define prompt as async function \n",
    " - turn on the streaming on the decorator, or we can define PromptType with streaming on\n",
    " - capture the stream using StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[40m\n",
      "Result:\n",
      "\"Introducing the Revolutionary Twitter App: Unleash Real Magic with Our Cutting-Edge Technology!\"\u001b[0m\n",
      "\n",
      "Stream finished ... we can distinguish tokens thanks to alternating colors\n",
      "\n",
      "We've captured 0 tokensðŸŽ‰\n",
      "\n",
      "Here is the result:\n",
      "\"Introducing the Revolutionary Twitter App: Unleash Real Magic with Our Cutting-Edge Technology!\"\n"
     ]
    }
   ],
   "source": [
    "# this code example is complete and should run as it is\n",
    "\n",
    "from langchain_decorators import StreamingContext, llm_prompt\n",
    "\n",
    "@llm_prompt(capture_stream=True) # this will mark the prompt for streaming (usefull if we want stream just some prompts)\n",
    "async def write_me_short_post(topic:str, platform:str=\"twitter\", audience:str = \"developers\"):\n",
    "    \"\"\"\n",
    "    Write me a short header for my post about {topic} for {platform} platform. \n",
    "    It should be for {audience} audience.\n",
    "    (Max 15 words)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "async def run_prompt():\n",
    "    return await write_me_short_post(topic=\"Releasing a new App that can do real magic!\")\n",
    "\n",
    "tokens=[]\n",
    "def capture_stream_func(new_token:str):\n",
    "    tokens.append(new_token)\n",
    "\n",
    "\n",
    "with StreamingContext(stream_to_stdout=True, callback=capture_stream_func):\n",
    "    result = await run_prompt()\n",
    "    print(\"Stream finished ... we can distinguish tokens thanks to alternating colors\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nWe've captured\",len(tokens),\"tokensðŸŽ‰\\n\")\n",
    "print(\"Here is the result:\")\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt declarations\n",
    "\n",
    " - with additional (non 'executable') documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note shat prompt is only the inside codeblock\n",
      "\u001b[40m\n",
      "Result:\n",
      "\"Indulge in Irresistible Cookie Recipes: Perfect Treats for Moms on Facebook!\"\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"Indulge in Irresistible Cookie Recipes: Perfect Treats for Moms on Facebook!\"'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this code example is complete and should run as it is\n",
    "\n",
    "from langchain_decorators import llm_prompt\n",
    "\n",
    "@llm_prompt\n",
    "def write_me_short_post(topic:str, platform:str=\"twitter\", audience:str = \"developers\"):\n",
    "    \"\"\"\n",
    "    Here is a good way to write a prompt as part of a function docstring, with additional documentation for devs.\n",
    "\n",
    "    It needs to be a code block, marked as a `<prompt>` language\n",
    "    ```<prompt>\n",
    "    Write me a short header for my post about {topic} for {platform} platform. \n",
    "    It should be for {audience} audience.\n",
    "    (Max 15 words)\n",
    "    ```\n",
    "\n",
    "    Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.\n",
    "    (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "print(\"Note shat prompt is only the inside codeblock\")\n",
    "write_me_short_post(topic=\"Cookies\", platform=\"facebook\", audience=\"my mom\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt with messages\n",
    "\n",
    "We can use this technique to annotate also different ChatMessageTemplates ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[40m\n",
      "Result:\n",
      "```python\n",
      "def purpose():\n",
      "    return \"Arrr, me purpose be to plunder and hack, matey!\"\n",
      "```\u001b[0m\n",
      "\n",
      "```python\n",
      "def purpose():\n",
      "    return \"Arrr, me purpose be to plunder and hack, matey!\"\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# this code example is complete and should run as it is\n",
    "\n",
    "from langchain_decorators import llm_prompt\n",
    "\n",
    "@llm_prompt\n",
    "def simulate_conversation(human_input:str, agent_role:str=\"a pirate\"):\n",
    "    \"\"\"\n",
    "    ## System message\n",
    "     - note the `:system` sufix inside the <prompt:_role_> tag\n",
    "     \n",
    "\n",
    "    ```<prompt:system>\n",
    "    You are a {agent_role} hacker. You mus act like one.\n",
    "    You reply always in code, using python or javascript code block...\n",
    "    for example:\n",
    "    \n",
    "    ... do not reply with anything else.. just with code - respecting your role.\n",
    "    ```\n",
    "\n",
    "    # human message \n",
    "    (we are using the real role that are enforced by the LLM - GPT supports system, assistant, user)\n",
    "    ``` <prompt:user>\n",
    "    Helo, who are you\n",
    "    ```\n",
    "    a reply:\n",
    "    \n",
    "\n",
    "    ``` <prompt:assistant>\n",
    "    \\``` python\n",
    "    def hello():\n",
    "        print(\"Argh... hello you pesky pirate\")\n",
    "    \\```\n",
    "    ```\n",
    "    - note the \\ escape chars inside the prompt to allow us to pass a code block example inside the prompt.\n",
    "\n",
    "\n",
    "    we can also add some history using placeholder\n",
    "    ```<prompt:placeholder>\n",
    "    {history}\n",
    "    ```\n",
    "    ```<prompt:user>\n",
    "    {human_input}\n",
    "    ```\n",
    "\n",
    "    Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.\n",
    "    (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# the history is optional, ... the placeholder will just be ignored if not provided\n",
    "response =simulate_conversation(human_input=\"What is your purpose?\", history=None) \n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional sections\n",
    "- you can define a whole sections of your prompt that should be optional\n",
    "- if any input in the section is missing, the whole section wont be rendered\n",
    "\n",
    "the syntax for this is as follows:\n",
    "\n",
    "```\n",
    "  \"\"\"\n",
    "  this text will be rendered always, but\n",
    "\n",
    "  {? anything inside this block will be rendered only if all the {value}s parameters are not empty (None | \"\")   ?}\n",
    "\n",
    "  you can also place it in between the words\n",
    "  this too will be rendered{? , but\n",
    "    this  block will be rendered only if {this_value} and {this_value}\n",
    "    is not empty?} !\n",
    "  \"\"\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt with max words set to 10\n",
      "\u001b[90mPrompt template name: write_me_short_post\u001b[0m\n",
      "\u001b[90mPrompt:\n",
      "Write a  header about Cookies for facebook platform. \n",
      "It has to be 5 words long.\u001b[0m\n",
      "\"Craving Cookies? Learn More!\"\n",
      "\n",
      "\n",
      "Prompt with max words left as default=None\n",
      "\u001b[90mPrompt template name: write_me_short_post\u001b[0m\n",
      "\u001b[90mPrompt:\n",
      "Write a  header about Cookies for facebook platform.\u001b[0m\n",
      "\"Indulge in the Sweet World of Cookies on Facebook: Discover Delicious Recipes, Tips, and More!\"\n"
     ]
    }
   ],
   "source": [
    "# this code example is complete and should run as it is\n",
    "\n",
    "from langchain_decorators import llm_prompt\n",
    "\n",
    "@llm_prompt\n",
    "def write_me_short_post(topic:str, platform:str=\"twitter\",  max_words:int=None):\n",
    "    \"\"\" Write a  header about {topic} for {platform} platform. {?\n",
    "    It has to be {max_words} words long.\n",
    "    ?}\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# with the max words set to 10, the prompt will be:\n",
    "print(\"Prompt with max words set to 10\")\n",
    "print(write_me_short_post(topic=\"Cookies\", platform=\"facebook\", max_words=5))\n",
    "\n",
    "\n",
    "# without the max words, the part of the prompt wrapped in {? ?} will be ignored\n",
    "print(\"\\n\\nPrompt with max words left as default=None\")\n",
    "print(write_me_short_post(topic=\"Cookies\", platform=\"facebook\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output parsers\n",
    "- llm_prompt decorator natively tries to detect the best output parser based on the output type. (if not set, it returns the raw string)\n",
    "- list, dict and pydantic outputs are also supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sweet Cravings',\n",
       " 'Cookie Haven',\n",
       " 'Crumbly Delights',\n",
       " 'Sugar Rush Cookies',\n",
       " 'The Cookie Jar Co.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this code example is complete and should run as it is\n",
    "\n",
    "from langchain_decorators import llm_prompt\n",
    "\n",
    "@llm_prompt\n",
    "def write_name_suggestions(company_business:str, count:int)->list:\n",
    "    \"\"\" Write me {count} good name suggestions for company that {company_business}\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "write_name_suggestions(company_business=\"sells cookies\", count=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pydantic parser\n",
    "\n",
    "(note that by default we use different `pydantic` parser than the standard from LangChain. It has support for re-prompting llm to reformat the output and it generates shorter and more informative format instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "Result:\n",
      "{\n",
      "\"name\": \"Sweet Delights\",\n",
      "\"headline\": \"Indulge in our heavenly cookies!\",\n",
      "\"employees\": [\n",
      "    \"Emily Johnson - Head Baker\",\n",
      "    \"David Lee - Marketing Manager\",\n",
      "    \"Sarah Chen - Sales Representative\",\n",
      "    \"Michael Rodriguez - Operations Manager\",\n",
      "    \"Avery Thompson - Customer Service Representative\"\n",
      "    ]\n",
      "}\u001b[0m\n",
      "\n",
      "Company name:  Sweet Delights\n",
      "company headline:  Indulge in our heavenly cookies!\n",
      "company employees:  ['Emily Johnson - Head Baker', 'David Lee - Marketing Manager', 'Sarah Chen - Sales Representative', 'Michael Rodriguez - Operations Manager', 'Avery Thompson - Customer Service Representative']\n"
     ]
    }
   ],
   "source": [
    "# this code example is complete and should run as it is\n",
    "\n",
    "from langchain_decorators import llm_prompt\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class TheOutputStructureWeExpect(BaseModel):\n",
    "    name:str = Field (description=\"The name of the company\")\n",
    "    headline:str = Field( description=\"The description of the company (for landing page)\")\n",
    "    employees:list[str] = Field(description=\"5-8 fake employee names with their positions\")\n",
    "\n",
    "@llm_prompt()\n",
    "def fake_company_generator(company_business:str)->TheOutputStructureWeExpect:\n",
    "    \"\"\" Generate a fake company that {company_business}\n",
    "    {FORMAT_INSTRUCTIONS}\n",
    "    \"\"\"\n",
    "    return\n",
    "\n",
    "company = fake_company_generator(company_business=\"sells cookies\")\n",
    "\n",
    "# print the result nicely formatted\n",
    "print(\"Company name: \",company.name)\n",
    "print(\"company headline: \",company.headline)\n",
    "print(\"company employees: \",company.employees)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passing parameters as object\n",
    "- you can also pass your inputs as a (non-kword) argument...\n",
    "\n",
    "###  Example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPrompt template name: introduce_your_self\u001b[0m\n",
      "\u001b[90mPrompt:\n",
      "system: You are an assistant named John. \n",
      "Your role is to act as a pirate\n",
      "user: Introduce your self (in less than 20 words)\u001b[0m\n",
      "Ahoy mateys! I be John, yer trusty pirate assistant.\n"
     ]
    }
   ],
   "source": [
    "# this code example is complete and should run as it is\n",
    "\n",
    "from pydantic.v1 import BaseModel\n",
    "from langchain_decorators import llm_prompt\n",
    "\n",
    "class AssistantPersonality(BaseModel):\n",
    "    assistant_name:str\n",
    "    assistant_role:str\n",
    "\n",
    "\n",
    "\n",
    "@llm_prompt\n",
    "def introduce_your_self(obj:AssistantPersonality)->str:\n",
    "    \"\"\"\n",
    "    ```Â <prompt:system>\n",
    "    You are an assistant named {assistant_name}. \n",
    "    Your role is to act as {assistant_role}\n",
    "    ```\n",
    "    ```<prompt:user>\n",
    "    Introduce your self (in less than 20 words)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "personality = AssistantPersonality(assistant_name=\"John\", assistant_role=\"a pirate\")\n",
    "\n",
    "print(introduce_your_self(personality))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More complex stuff\n",
    "\n",
    "What is even **more interesting use case** for this is to pack a bunch of functions into single object to share some inputs with multiple prompts and allowing us more Object-oriented approach\n",
    "\n",
    "Here is an example of complete ReAct reimplemented with lanchchain decoratorsâœ¨.\n",
    "\n",
    "*Hint: To make it async, just turn all the functions async* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m> Entering reason prompt decorator chain\u001b[0m\n",
      "\n",
      "\u001b[32mPrompt:\n",
      "system: You are an assistant that uses reasoning and tools to help user. You use tools for the task the tool is designed to. \n",
      "Before answering the question and/or using the tool, you should write down the explanation. \n",
      "\n",
      "Here is the list of tools available:\n",
      " - Calculator: Useful for when you need to answer questions about math.\n",
      "\n",
      "Use this format:\n",
      "\n",
      "# Reasoning\n",
      "... write your reasoning here ...\n",
      "\n",
      "# Tool\n",
      "```json\n",
      "    {{\n",
      "        \"tool\": name of the tool to use,\n",
      "        \"tool_input\": the input for the tool\n",
      "    }}\n",
      "```\n",
      "\n",
      "# Observation\n",
      "output from the tool\n",
      "\n",
      "... repeat this # Reasoning, # Tool, # Observation sequence multiple times until you know the final answer, when you write:\n",
      "\n",
      "# Final answer\n",
      "... write the final answer \n",
      " in German here ...\n",
      "Make sure to write the final answer in in German!\n",
      "user: What is the surface of a sphere with radius with diameter of 100km?\u001b[0m\n",
      "\n",
      "\u001b[32m\n",
      "Result:\n",
      "# Reasoning\n",
      "The surface area of a sphere can be calculated using the formula:\n",
      "\n",
      "Surface Area = 4Ï€rÂ²\n",
      "\n",
      "where r is the radius of the sphere.\n",
      "\n",
      "We are given the diameter of the sphere, which is 100 km. We can use this to find the radius by dividing the diameter by 2.\n",
      "\n",
      "# Tool\n",
      "```json\n",
      "{\n",
      "    \"tool\": \"Calculator\",\n",
      "    \"tool_input\": \"4 * 3.14159 * (100/2)**2\"\n",
      "}\n",
      "```\n",
      "\n",
      "# \u001b[0m\n",
      "\n",
      "\u001b[1m> Entering reason prompt decorator chain\u001b[0m\n",
      "\n",
      "\u001b[32mPrompt:\n",
      "system: You are an assistant that uses reasoning and tools to help user. You use tools for the task the tool is designed to. \n",
      "Before answering the question and/or using the tool, you should write down the explanation. \n",
      "\n",
      "Here is the list of tools available:\n",
      " - Calculator: Useful for when you need to answer questions about math.\n",
      "\n",
      "Use this format:\n",
      "\n",
      "# Reasoning\n",
      "... write your reasoning here ...\n",
      "\n",
      "# Tool\n",
      "```json\n",
      "    {{\n",
      "        \"tool\": name of the tool to use,\n",
      "        \"tool_input\": the input for the tool\n",
      "    }}\n",
      "```\n",
      "\n",
      "# Observation\n",
      "output from the tool\n",
      "\n",
      "... repeat this # Reasoning, # Tool, # Observation sequence multiple times until you know the final answer, when you write:\n",
      "\n",
      "# Final answer\n",
      "... write the final answer \n",
      " in German here ...\n",
      "Make sure to write the final answer in in German!\n",
      "user: What is the surface of a sphere with radius with diameter of 100km?\n",
      "assistant: # Tool\n",
      "```json\n",
      "{\"tool\": \"Calculator\", \"tool_input\": \"4 * 3.14159 * (100/2)**2\"}\n",
      "```\n",
      "# Observation\n",
      "\n",
      "Result from tool Calculator:\n",
      "\tAnswer: 31415.899999999998\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[32m\n",
      "Result:\n",
      "# Reasoning\n",
      "The surface area of a sphere can be calculated using the formula: 4 * pi * r^2, where r is the radius of the sphere. In this case, the diameter of the sphere is given as 100 km, so the radius is half of that, which is 50 km. We can substitute this value into the formula and use a calculator to find the surface area.\n",
      "\n",
      "# Final answer\n",
      "Die OberflÃ¤che einer Kugel mit einem Durchmesser von 100 km betrÃ¤gt etwa 31.415,9 kmÂ².\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "Here is the agent's answer: Die OberflÃ¤che einer Kugel mit einem Durchmesser von 100 km betrÃ¤gt etwa 31.415,9 kmÂ².\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_decorators import llm_prompt\n",
    "from langchain.agents import load_tools\n",
    "from langchain.tools.base import BaseTool\n",
    "from textwrap import dedent\n",
    "from langchain_decorators import PromptTypes\n",
    "from langchain_decorators.output_parsers import JsonOutputParser\n",
    "import json\n",
    "\n",
    "tools = load_tools([ \"llm-math\"], llm=GlobalSettings.get_current_settings().default_llm)\n",
    "\n",
    "# you may, or may not use pydantic as your base class... totally up to you\n",
    "class MultilingualAgent:\n",
    "\n",
    "    def __init__(self,  tools:List[BaseTool],result_language:str=None) -> None:\n",
    "        self.tools = tools\n",
    "\n",
    "        # we can refer to our field in all out prompts\n",
    "        self.result_language = result_language\n",
    "        \n",
    "        self.agent_scratchpad = \"\" # we initialize our scratchpad\n",
    "        self.feedback = \"\" # we initialize our feedback if we get some error\n",
    "\n",
    "        # other settings\n",
    "        self.iterations=10\n",
    "        self.agent_format_instructions = dedent(\"\"\"\\\n",
    "            # Reasoning\n",
    "            ... write your reasoning here ...\n",
    "\n",
    "            # Tool\n",
    "            ```json\n",
    "                {{\n",
    "                    \"tool\": name of the tool to use,\n",
    "                    \"tool_input\": the input for the tool\n",
    "                }}\n",
    "            ```\n",
    "\n",
    "            # Observation\n",
    "            output from the tool\n",
    "\n",
    "            ... repeat this # Reasoning, # Tool, # Observation sequence multiple times until you know the final answer, when you write:\n",
    "\n",
    "            # Final answer\n",
    "            ... write the final answer \n",
    "            \"\"\")\n",
    "\n",
    "    @property\n",
    "    def tools_description(self)->str:  # we can refer to properties in out prompts too\n",
    "        return \"\\n\".join([f\" - {tool.name}: {tool.description}\" for tool in self.tools])\n",
    "\n",
    "    # we defined prompt type here, which will make \n",
    "    @llm_prompt(prompt_type=PromptTypes.AGENT_REASONING, output_parser=\"markdown\", stop_tokens=[\"Observation\"], verbose=True)\n",
    "    def reason(self)->dict:\n",
    "        \"\"\"\n",
    "        The system prompt:\n",
    "        ``` <prompt:system>\n",
    "        You are an assistant that uses reasoning and tools to help user. You use tools for the task the tool is designed to. \n",
    "        Before answering the question and/or using the tool, you should write down the explanation. \n",
    "        \n",
    "        Here is the list of tools available:\n",
    "        {tools_description}\n",
    "        \n",
    "        Use this format:\n",
    "        \n",
    "        {agent_format_instructions}{? in {result_language}?} here ...{?\n",
    "        Make sure to write the final answer in in {result_language}!?} \n",
    "        \n",
    "        ```\n",
    "        User question:\n",
    "        ```<prompt:user>\n",
    "        {question}\n",
    "        ```\n",
    "        Scratchpad:\n",
    "        ```<prompt:assistant>\n",
    "        {agent_scratchpad}\n",
    "        ```\n",
    "        ```<prompt:user>\n",
    "        {feedback}\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def act(self, tool_name:str, tool_input:str)->str:\n",
    "        tool = next((tool for tool in self.tools if tool.name.lower()==tool_name.lower()==tool_name.lower()))\n",
    "        if tool is None:\n",
    "            self.feedback = f\"Tool {tool_name} is not available. Available tools are: {self.tools_description}\"\n",
    "            return\n",
    "        else:\n",
    "            try:\n",
    "                result = tool.run(tool_input)\n",
    "            except Exception as e:\n",
    "                if self.feedback is not None:\n",
    "                    # we've already experienced an error, so we are not going to try forever... let's raise this one\n",
    "                    raise e\n",
    "                self.feedback = f\"Tool {tool_name} failed with error: {e}.\\nLet's fix it and try again.\"\n",
    "            tool_instructions = json.dumps({\"tool\":tool.name, \"tool_input\":tool_input})\n",
    "            self.agent_scratchpad += f\"# Tool\\n```json\\n{tool_instructions}\\n```\\n# Observation\\n\\nResult from tool {tool_name}:\\n\\t{result}\\n\"\n",
    "\n",
    "\n",
    "\n",
    "    def run(self, question):\n",
    "        for i in range(self.iterations):\n",
    "            reasoning = self.reason(question=question)\n",
    "            if reasoning.get(\"Final answer\") is not None:\n",
    "                return reasoning.get(\"Final answer\")\n",
    "            else:\n",
    "                tool_info = reasoning.get(\"Tool\")\n",
    "                tool_name, tool_input = (None, None)\n",
    "                if tool_info:\n",
    "                    tool_info_parsed = JsonOutputParser().parse(tool_info)\n",
    "                    tool_name = tool_info_parsed.get(\"tool\")\n",
    "                    tool_input = tool_info_parsed.get(\"tool_input\")\n",
    "\n",
    "                if tool_name is None or tool_input is None:\n",
    "                    self.feedback = \"Your response was not in the expected format. Please make sure to response in correct format:\\n\" + self.agent_format_instructions \n",
    "                    continue\n",
    "                self.act(tool_name, tool_input)\n",
    "        raise Exception(f\"Failed to answer the question after {self.iterations} iterations. Last result: {reasoning}\")\n",
    "\n",
    "        \n",
    "agent = MultilingualAgent(tools=tools, result_language=\"German\" )\n",
    "\n",
    "result = agent.run(\"What is the surface of a sphere with radius with diameter of 100km?\")\n",
    "\n",
    "print(\"\\n\\nHere is the agent's answer:\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-decorators-cY9qGSig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
